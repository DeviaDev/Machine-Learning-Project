{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4fa6f4",
   "metadata": {},
   "source": [
    "# Iris Classification Model\n",
    "\n",
    "Laporan akhir ini merangkum seluruh tahapan pembuatan model klasifikasi bunga Iris, mulai dari persiapan data, pelatihan model, visualisasi hasil, hingga prediksi data baru. Notebook ini dapat dijalankan secara berurutan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5aeed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bbbb1",
   "metadata": {},
   "source": [
    "## 1. Persiapan Data\n",
    "Memuat dataset Iris bawaan scikit-learn, membagi data menjadi latih dan uji, serta menyimpan ke file CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "df = pd.concat([iris.data, iris.target.rename('target')], axis=1)\n",
    "\n",
    "# Simpan data\n",
    "df.to_csv('iris_full.csv', index=False)\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['target'])\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)\n",
    "\n",
    "print(\"Data disiapkan: train.csv, test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21aa642",
   "metadata": {},
   "source": [
    "## 2. Pelatihan Model\n",
    "Membaca `train.csv`, melakukan scaling, dan melatih model MLP selama 100 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisi model\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=16, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Load and preprocess\n",
    "df_train = pd.read_csv('train.csv')\n",
    "X = df_train.drop(columns='target').values\n",
    "y = df_train['target'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train, X_val = scaler.transform(X_train), scaler.transform(X_val)\n",
    "\n",
    "# Simpan scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# DataLoader\n",
    "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_ds   = TensorDataset(torch.tensor(X_val, dtype=torch.float32),   torch.tensor(y_val, dtype=torch.long))\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16)\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model      = IrisNet().to(device)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "train_losses, val_accs = [], []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs} - Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'final_model.pth')\n",
    "print(f\"Training selesai. Best Val Acc = {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4f35b",
   "metadata": {},
   "source": [
    "## 3. Visualisasi Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss dan Akurasi\n",
    "epochs = range(1, n_epochs+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses)\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, val_accs)\n",
    "plt.title('Validation Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec6952",
   "metadata": {},
   "source": [
    "## 4. Prediksi Data Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh data baru\n",
    "new_samples = np.array([[5.1, 3.5, 1.4, 0.2],\n",
    "                        [6.7, 3.0, 5.2, 2.3]])\n",
    "\n",
    "# Load scaler & model\n",
    "scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "model = IrisNet().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Preprocess & predict\n",
    "X_new = scaler.transform(new_samples)\n",
    "with torch.no_grad():\n",
    "    preds = model(torch.tensor(X_new, dtype=torch.float32).to(device)).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "# Tampilkan hasil\n",
    "for sample, p in zip(new_samples, preds):\n",
    "    print(f\"Features: {sample} -> Predicted class: {iris.target_names[p]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
